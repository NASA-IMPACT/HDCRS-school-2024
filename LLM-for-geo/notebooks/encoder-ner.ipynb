{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e14897-bd7d-40ca-ab65-4ddd2646c0af",
   "metadata": {},
   "source": [
    "# Datetime and Location extraction using Finetuned Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578c614-816d-46e8-bbe6-37587b504422",
   "metadata": {},
   "source": [
    "## Install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7ec18b-609b-41df-93cc-9642bae51d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0.post104)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets accelerate\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02078472-f75f-4c93-8cec-9a63946aa1e2",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f352e1-5476-4a1b-8ea3-fa8eef5973fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 07:48:29.598996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/encoder-ner.csv', converters={'tokens': eval, 'ner_tags': eval})\n",
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc0cf45-7c0e-4d08-8428-d3d5b52ecde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c87728-8801-4242-89d7-99bbc62cfd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Satellite',\n",
       "  'images',\n",
       "  'of',\n",
       "  'burn',\n",
       "  'scars',\n",
       "  'in',\n",
       "  'Montana',\n",
       "  'from',\n",
       "  'August',\n",
       "  '14',\n",
       "  ',',\n",
       "  '2023',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ad4898c-cd85-46b9-ace4-b25e99986542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format the data for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762ee4e-5352-4691-9ad2-960f7607ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unique labels\n",
    "unique_labels = ['O', 'DATE', 'LOCATION']  # add all your labels here\n",
    "label_dict = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Update the dataset with encoded labels\n",
    "def encode_labels(examples):\n",
    "    try:\n",
    "        return {'ner_tags': [label_dict[label] for label in examples['ner_tags']]}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "data = data.map(encode_labels)\n",
    "\n",
    "def convert_list(input_list):\n",
    "    output_list = input_list\n",
    "    for i in range(2, len(output_list) - 1):\n",
    "        if output_list[i - 2] == 1 and output_list[i - 1] == 1 and output_list[i] == 0 and output_list[i + 1] == 1:\n",
    "            output_list[i] = 1\n",
    "            break\n",
    "    return output_list\n",
    "\n",
    "\n",
    "\n",
    "def consolidate_labels(dataset):\n",
    "    dataset[\"ner_tags\"] = convert_list(dataset[\"ner_tags\"])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "data = data.map(consolidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68dfec30-e6bb-4b63-a0ac-5755138a1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf82ddf-d6ee-43ba-9be7-ba69c56c77de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b676106eee4a65829c204fc245ae0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, None]\n",
      "[0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "[0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        print(label, word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            try:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx  # Update previous_word_idx inside the loop\n",
    "            except Exception as e:  # Use Exception instead of error\n",
    "                continue\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, batch_size=128)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0399160-e78c-4ea1-a121-40d4163c2755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Burn',\n",
       "  'scars',\n",
       "  'in',\n",
       "  'Oregon',\n",
       "  ',',\n",
       "  'noted',\n",
       "  'on',\n",
       "  'April',\n",
       "  '17',\n",
       "  ',',\n",
       "  '2023',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0],\n",
       " 'input_ids': [0,\n",
       "  7960,\n",
       "  26172,\n",
       "  11,\n",
       "  4316,\n",
       "  2156,\n",
       "  1581,\n",
       "  15,\n",
       "  587,\n",
       "  601,\n",
       "  2156,\n",
       "  291,\n",
       "  1922,\n",
       "  479,\n",
       "  2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 1, 0, -100]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e687699-538c-4159-ae6c-aca23803801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Huggingface Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace9dbb2-2485-431a-8f3c-fc61f24043b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=len(unique_labels) # This should match your total number of NER tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1781c50d-5d72-4da7-a640-8ee830377cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b17512-5dc1-4158-9ea6-4012523df45a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.14.343, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 02:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.188638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.128659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.086164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.046331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.021326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.007152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=0.13096021505502553, metrics={'train_runtime': 138.3168, 'train_samples_per_second': 7.157, 'train_steps_per_second': 0.94, 'total_flos': 8461600380360.0, 'train_loss': 0.13096021505502553, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming you have a train split\n",
    "    eval_dataset=tokenized_datasets,  # Assuming you have a train split\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e2e6ba3-0659-4a76-a153-b21337ee5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a429170d-b19b-4d38-8581-ba441dea4399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_roberta_ner/tokenizer_config.json',\n",
       " './finetuned_roberta_ner/special_tokens_map.json',\n",
       " './finetuned_roberta_ner/vocab.json',\n",
       " './finetuned_roberta_ner/merges.txt',\n",
       " './finetuned_roberta_ner/added_tokens.json',\n",
       " './finetuned_roberta_ner/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./finetuned_roberta_ner')\n",
    "tokenizer.save_pretrained('./finetuned_roberta_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "310b5cf6-809c-47a7-8a71-a2c67fe40172",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba939e5e-4cf5-452e-9bd0-0434aad6e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   230,  6884,  3505,    11,  2809, 17373,   179,   883,   587,\n",
      "             6,   291,  1922,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input_ids tensor([[    0,   230,  6884,  3505,    11,  2809, 17373,   179,   883,   587,\n",
      "             6,   291,  1922,     4,     2]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Crop types in Spain durin 23 April, 2023.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "print(inputs)\n",
    "for k, v in inputs.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23e09d3b-d18b-42c9-b2c3-4a995a62fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25f0bd33-ffcc-4f45-898f-3cacbca258e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.99981743, 'index': 1, 'word': 'Ä C', 'start': 0, 'end': 1}, {'entity': 'LABEL_0', 'score': 0.99984944, 'index': 2, 'word': 'rop', 'start': 1, 'end': 4}, {'entity': 'LABEL_0', 'score': 0.9998709, 'index': 3, 'word': 'Ä types', 'start': 5, 'end': 10}, {'entity': 'LABEL_0', 'score': 0.9998066, 'index': 4, 'word': 'Ä in', 'start': 11, 'end': 13}, {'entity': 'LABEL_2', 'score': 0.9999361, 'index': 5, 'word': 'Ä Spain', 'start': 14, 'end': 19}, {'entity': 'LABEL_0', 'score': 0.9998373, 'index': 6, 'word': 'Ä dur', 'start': 20, 'end': 23}, {'entity': 'LABEL_0', 'score': 0.99967325, 'index': 7, 'word': 'in', 'start': 23, 'end': 25}, {'entity': 'LABEL_1', 'score': 0.9992142, 'index': 8, 'word': 'Ä 23', 'start': 26, 'end': 28}, {'entity': 'LABEL_1', 'score': 0.99935466, 'index': 9, 'word': 'Ä April', 'start': 29, 'end': 34}, {'entity': 'LABEL_1', 'score': 0.99947006, 'index': 10, 'word': ',', 'start': 34, 'end': 35}, {'entity': 'LABEL_1', 'score': 0.9980672, 'index': 11, 'word': 'Ä 20', 'start': 36, 'end': 38}, {'entity': 'LABEL_1', 'score': 0.9973851, 'index': 12, 'word': '23', 'start': 38, 'end': 40}, {'entity': 'LABEL_0', 'score': 0.99443495, 'index': 13, 'word': '.', 'start': 40, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "ner_classes = ner_pipeline(text)\n",
    "print(ner_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c098df6b-3ded-44f9-a523-88b246452609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.9996712, 'index': 1, 'word': 'Ä C', 'start': 0, 'end': 1}, {'entity': 'LABEL_0', 'score': 0.99229616, 'index': 2, 'word': 'rop', 'start': 1, 'end': 4}, {'entity': 'LABEL_0', 'score': 0.9997447, 'index': 3, 'word': 'Ä types', 'start': 5, 'end': 10}, {'entity': 'LABEL_0', 'score': 0.9997352, 'index': 4, 'word': 'Ä in', 'start': 11, 'end': 13}, {'entity': 'LABEL_2', 'score': 0.99991083, 'index': 5, 'word': 'Ä Spain', 'start': 14, 'end': 19}, {'entity': 'LABEL_0', 'score': 0.9928919, 'index': 6, 'word': 'Ä as', 'start': 20, 'end': 22}, {'entity': 'LABEL_0', 'score': 0.87052906, 'index': 7, 'word': 'Ä of', 'start': 23, 'end': 25}, {'entity': 'LABEL_0', 'score': 0.98498046, 'index': 8, 'word': 'Ä yesterday', 'start': 26, 'end': 35}, {'entity': 'LABEL_0', 'score': 0.99229455, 'index': 9, 'word': '.', 'start': 35, 'end': 36}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Crop types in Spain as of yesterday.\"\n",
    "ner_classes = ner_pipeline(text)\n",
    "print(ner_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4877b1d3-d16c-4228-acbe-b5c6851a4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.9996712,\n",
       "  'index': 1,\n",
       "  'word': 'Ä C',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99229616,\n",
       "  'index': 2,\n",
       "  'word': 'rop',\n",
       "  'start': 1,\n",
       "  'end': 4},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9997447,\n",
       "  'index': 3,\n",
       "  'word': 'Ä types',\n",
       "  'start': 5,\n",
       "  'end': 10},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9997352,\n",
       "  'index': 4,\n",
       "  'word': 'Ä in',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99991083,\n",
       "  'index': 5,\n",
       "  'word': 'Ä Spain',\n",
       "  'start': 14,\n",
       "  'end': 19},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9928919,\n",
       "  'index': 6,\n",
       "  'word': 'Ä as',\n",
       "  'start': 20,\n",
       "  'end': 22},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.87052906,\n",
       "  'index': 7,\n",
       "  'word': 'Ä of',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.98498046,\n",
       "  'index': 8,\n",
       "  'word': 'Ä yesterday',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99229455,\n",
       "  'index': 9,\n",
       "  'word': '.',\n",
       "  'start': 35,\n",
       "  'end': 36}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1d48c11-870d-4ee1-8b76-18e6ccddb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOCATION': 'Spain', 'DATE': '23 April, 2023'}\n"
     ]
    }
   ],
   "source": [
    "## Conver NER format to JSON format\n",
    "\n",
    "def ner_to_dict(ner_result):\n",
    "    result = dict()\n",
    "    for item in ner_result:\n",
    "        if item['entity'] in [\"LABEL_1\", \"LABEL_2\"]:\n",
    "            if item['entity'] in result.keys():\n",
    "                result[item['entity']] += item['word']\n",
    "            else:\n",
    "                result[item['entity']] = \"\"\n",
    "                result[item['entity']] += item['word']\n",
    "    old_keys = list(result.keys())\n",
    "    \n",
    "    for key in old_keys:\n",
    "        new_key = unique_labels[int(key[-1])]\n",
    "        result[new_key] = result[key]\n",
    "        del result[key]\n",
    "    \n",
    "    for key, val in result.items():\n",
    "        result[key] = val.replace('Ä ', ' ')[1:]\n",
    "\n",
    "    return result\n",
    "\n",
    "print(ner_to_dict(ner_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0296834-1ac3-44a8-8de4-8ad59462ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
