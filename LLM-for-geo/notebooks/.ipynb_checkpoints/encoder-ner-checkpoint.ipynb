{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7ec18b-609b-41df-93cc-9642bae51d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /p/software/jurecadc/stages/2023/software/SciPy-bundle/2022.05-gcccoremkl-11.3.0-2022.1.0/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /p/software/jurecadc/stages/2023/software/PyYAML/6.0-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: requests in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0\n",
      "  Using cached huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "Requirement already satisfied: filelock in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (2022.4.24)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: pandas in /p/software/jurecadc/stages/2023/software/SciPy-bundle/2022.05-gcccoremkl-11.3.0-2022.1.0/lib/python3.10/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: aiohttp in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Using cached pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Collecting pyarrow-hotfix\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /p/software/jurecadc/stages/2023/software/JupyterLab/2023.3.6-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /p/software/jurecadc/stages/2023/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, safetensors, requests, pyarrow-hotfix, pyarrow, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages/xxhash'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/p/software/jurecadc/stages/2023/software/Python/3.10.4-GCCcore-11.3.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers datasets\n",
    "\n",
    "import os\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/p/project/training2411/huggingface/roberta'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f352e1-5476-4a1b-8ea3-fa8eef5973fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, ClassLabel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizerFast\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForTokenClassification\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/ner_new.csv', converters={'tokens': eval, 'ner_tags': eval})\n",
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c87728-8801-4242-89d7-99bbc62cfd02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m[\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c757bd-702d-4b9b-b1b0-1247d452b124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f19c16f384ac9973380f3c61462c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de75cd4561064c1f91e5572ea662f1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define unique labels\n",
    "unique_labels = ['O', 'DATE', 'LOCATION']  # add all your labels here\n",
    "label_dict = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Update the dataset with encoded labels\n",
    "def encode_labels(examples):\n",
    "    try:\n",
    "        return {'ner_tags': [label_dict[label] for label in examples['ner_tags']]}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "data = data.map(encode_labels)\n",
    "\n",
    "def convert_list(input_list):\n",
    "    output_list = input_list\n",
    "    for i in range(2, len(output_list) - 1):\n",
    "        if output_list[i - 2] == 1 and output_list[i - 1] == 1 and output_list[i] == 0 and output_list[i + 1] == 1:\n",
    "            output_list[i] = 1\n",
    "            break\n",
    "    return output_list\n",
    "\n",
    "\n",
    "\n",
    "def consolidate_labels(dataset):\n",
    "    dataset[\"ner_tags\"] = convert_list(dataset[\"ner_tags\"])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "data = data.map(consolidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3862f15-2429-4eb5-9796-c2e87f61cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['February', '8', ',', '2023', ':', 'Provide', 'satellite', 'imagery', 'of', 'flooding', 'in', 'Miami', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Burn', 'scars', 'in', 'Oregon', ',', 'noted', 'on', 'April', '17', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['July', '25', ',', '2023', ':', 'Identify', 'crop', 'types', 'in', 'Northern', 'India', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'central', 'China', 'on', 'June', '30', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Satellite', 'images', 'of', 'burn', 'scars', 'in', 'Montana', 'from', 'August', '14', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['December', '5', ',', '2023', ':', 'Show', 'crop', 'types', 'in', 'Chile', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['March', '22', ',', '2023', ',', 'highlight', 'flooding', 'events', 'in', 'southern', 'France', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['On', 'November', '10', ',', '2023', ',', 'find', 'burn', 'scars', 'in', 'Victoria', ',', 'Australia', '.'], 'ner_tags': [0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Ukraine', ',', 'recorded', 'on', 'September', '3', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['October', '27', ',', '2023', ':', 'Flooding', 'in', 'Cairo', ',', 'Egypt', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['January', '9', ',', '2023', ':', 'Display', 'burn', 'scars', 'in', 'the', 'Scottish', 'Highlands', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Kansas', 'as', 'of', 'February', '16', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'Bangladesh', 'on', 'March', '5', ',', '2023', '.'], 'ner_tags': [0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['April', '28', ',', '2023', ',', 'show', 'burn', 'scars', 'in', 'New', 'Mexico', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['May', '15', ',', '2023', ':', 'Crop', 'types', 'in', 'Brazil', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['June', '11', ',', '2023', ':', 'Flooding', 'in', 'Houston', ',', 'Texas', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['September', '7', ',', '2023', ',', 'display', 'burn', 'scars', 'in', 'Arizona', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Egypt', 'observed', 'on', 'November', '2', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['December', '18', ',', '2023', ':', 'Flooding', 'in', 'Venice', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['January', '22', ',', '2023', ':', 'Burn', 'scars', 'in', 'British', 'Columbia', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['March', '19', ',', '2023', ':', 'Crop', 'types', 'in', 'Ethiopia', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['July', '14', ',', '2023', ',', 'flooding', 'in', 'Seoul', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Burn', 'scars', 'in', 'the', 'Amazon', 'rainforest', 'on', 'August', '26', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['October', '13', ',', '2023', ',', 'show', 'crop', 'types', 'in', 'Italy', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'the', 'Netherlands', 'on', 'February', '21', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['On', 'April', '4', ',', '2023', ',', 'display', 'burn', 'scars', 'in', 'California', '.'], 'ner_tags': [0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Morocco', 'as', 'of', 'May', '28', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'SÃ£o', 'Paulo', 'on', 'June', '19', ',', '2023', '.'], 'ner_tags': [0, 0, 2, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Satellite', 'imagery', 'of', 'burn', 'scars', 'in', 'Utah', 'from', 'September', '9', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['November', '23', ',', '2023', ':', 'Crop', 'types', 'in', 'Vietnam', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['December', '3', ',', '2023', ',', 'highlight', 'flooding', 'events', 'in', 'Manila', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['January', '31', ',', '2023', ':', 'Burn', 'scars', 'in', 'Nevada', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Poland', ',', 'noted', 'on', 'March', '24', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'Malaysia', 'on', 'July', '8', ',', '2023', '.'], 'ner_tags': [0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Satellite', 'images', 'of', 'burn', 'scars', 'from', 'Siberia', 'on', 'August', '17', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Turkey', 'as', 'of', 'October', '6', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['November', '27', ',', '2023', ':', 'Flooding', 'in', 'London', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['January', '16', ',', '2023', ',', 'show', 'burn', 'scars', 'in', 'Tasmania', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Greece', 'observed', 'on', 'February', '25', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['On', 'March', '10', ',', '2023', ',', 'find', 'flooding', 'events', 'in', 'New', 'Orleans', '.'], 'ner_tags': [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['April', '20', ',', '2023', ':', 'Burn', 'scars', 'in', 'Portugal', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'South', 'Africa', 'from', 'May', '22', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'the', 'Philippines', 'on', 'June', '28', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['July', '12', ',', '2023', ',', 'display', 'burn', 'scars', 'in', 'South', 'Dakota', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Argentina', 'as', 'of', 'September', '15', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['October', '31', ',', '2023', ':', 'Flooding', 'in', 'Berlin', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Satellite', 'imagery', 'of', 'burn', 'scars', 'in', 'New', 'Zealand', 'from', 'December', '9', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['January', '4', ',', '2023', ':', 'Crop', 'types', 'in', 'Japan', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['March', '6', ',', '2023', ',', 'show', 'flooding', 'in', 'Alabama', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['April', '29', ',', '2023', ':', 'Burn', 'scars', 'in', 'the', 'Sahel', 'region', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Belgium', 'noted', 'on', 'May', '24', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'South', 'Korea', 'on', 'June', '15', ',', '2023', '.'], 'ner_tags': [0, 0, 2, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Burn', 'scars', 'in', 'Alaska', 'from', 'July', '21', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Romania', 'as', 'of', 'August', '30', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['September', '20', ',', '2023', ':', 'Flooding', 'in', 'Lima', ',', 'Peru', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['November', '5', ',', '2023', ',', 'display', 'burn', 'scars', 'in', 'the', 'Pyrenees', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['On', 'January', '15', ',', '2023', ',', 'display', 'the', 'flooding', 'events', 'in', 'Jakarta', '.'], 'ner_tags': [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Show', 'images', 'of', 'burn', 'scars', 'in', 'the', 'Colorado', 'Rockies', 'from', 'March', '12', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Sudan', 'as', 'observed', 'on', 'May', '20', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['February', '8', ',', '2023', ':', 'Provide', 'satellite', 'imagery', 'of', 'flooding', 'in', 'Miami', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Burn', 'scars', 'in', 'Oregon', ',', 'noted', 'on', 'April', '17', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['July', '25', ',', '2023', ':', 'Identify', 'crop', 'types', 'in', 'Northern', 'India', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'central', 'China', 'on', 'June', '30', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Satellite', 'images', 'of', 'burn', 'scars', 'in', 'Montana', 'from', 'August', '14', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['December', '5', ',', '2023', ':', 'Show', 'crop', 'types', 'in', 'Chile', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['March', '22', ',', '2023', ',', 'highlight', 'flooding', 'events', 'in', 'southern', 'France', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['On', 'November', '10', ',', '2023', ',', 'find', 'burn', 'scars', 'in', 'Victoria', ',', 'Australia', '.'], 'ner_tags': [0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Ukraine', ',', 'recorded', 'on', 'September', '3', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['October', '27', ',', '2023', ':', 'Flooding', 'in', 'Cairo', ',', 'Egypt', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['January', '9', ',', '2023', ':', 'Display', 'burn', 'scars', 'in', 'the', 'Scottish', 'Highlands', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['Crop', 'types', 'in', 'Kansas', 'as', 'of', 'February', '16', ',', '2023', '.'], 'ner_tags': [0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['Flooding', 'in', 'Bangladesh', 'on', 'March', '5', ',', '2023', '.'], 'ner_tags': [0, 0, 2, 0, 1, 1, 1, 1, 0]}\n",
      "{'tokens': ['April', '28', ',', '2023', ',', 'show', 'burn', 'scars', 'in', 'New', 'Mexico', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]}\n",
      "{'tokens': ['May', '15', ',', '2023', ':', 'Crop', 'types', 'in', 'Brazil', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['June', '11', ',', '2023', ':', 'Flooding', 'in', 'Houston', ',', 'Texas', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0]}\n",
      "{'tokens': ['September', '7', ',', '2023', ',', 'display', 'burn', 'scars', 'in', 'Arizona', '.'], 'ner_tags': [1, 1, 1, 1, 0, 0, 0, 0, 2, 0]}\n",
      "{'tokens': ['Provide', 'the', 'latest', 'imagery', 'of', 'flooding', 'in', 'Houston', ',', 'Texas', ',', 'from', 'this', 'past', 'Tuesday', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['Display', 'satellite', 'images', 'of', 'burn', 'scars', 'in', 'the', 'Alps', 'as', 'of', 'June', '2024', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Can', 'you', 'show', 'crop', 'types', 'in', 'Vietnam', 'observed', 'last', 'Friday', '?'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0]}\n",
      "{'tokens': ['Highlight', 'recent', 'flooding', 'events', 'in', 'the', 'UK', 'from', 'this', 'past', 'Spring', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['Show', 'images', 'of', 'burn', 'scars', 'in', 'Utah', 'as', 'of', 'the', 'first', 'week', 'of', 'May', '2024', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Find', 'current', 'crop', 'types', 'in', 'Poland', 'as', 'observed', 'today', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Display', 'the', 'latest', 'flooding', 'events', 'in', 'Toronto', ',', 'Canada', ',', 'from', 'last', 'month', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Show', 'satellite', 'imagery', 'of', 'burn', 'scars', 'in', 'Morocco', 'from', 'this', 'year', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Can', 'you', 'find', 'crop', 'types', 'in', 'Kansas', 'as', 'of', 'the', 'last', '30', 'days', '?'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Provide', 'images', 'of', 'recent', 'flooding', 'in', 'Cairo', ',', 'Egypt', ',', 'from', 'the', 'past', 'week', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Highlight', 'burn', 'scars', 'in', 'Tasmania', 'as', 'observed', 'last', 'Monday', '.'], 'ner_tags': [0, 0, 0, 0, 2, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['Display', 'the', 'most', 'recent', 'crop', 'types', 'in', 'Germany', 'from', 'this', 'Wednesday', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Show', 'recent', 'flooding', 'in', 'Miami', ',', 'Florida', ',', 'as', 'of', 'this', 'April', '.'], 'ner_tags': [0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Find', 'burn', 'scars', 'in', 'the', 'Kalahari', 'Desert', 'from', 'the', 'past', 'three', 'months', '.'], 'ner_tags': [0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Can', 'you', 'show', 'crop', 'types', 'in', 'the', 'Netherlands', 'observed', 'last', 'weekend', '?'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0]}\n",
      "{'tokens': ['Highlight', 'flooding', 'events', 'in', 'Lima', ',', 'Peru', ',', 'from', 'this', 'Spring', '.'], 'ner_tags': [0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['Display', 'current', 'burn', 'scars', 'in', 'South', 'Dakota', 'as', 'of', 'today', '.'], 'ner_tags': [0, 0, 0, 0, 2, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Show', 'the', 'most', 'recent', 'crop', 'types', 'in', 'Turkey', 'from', 'last', 'Tuesday', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Provide', 'satellite', 'images', 'of', 'flooding', 'in', 'Seoul', ',', 'South', 'Korea', ',', 'from', 'this', 'month', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 0]}\n",
      "{'tokens': ['Find', 'burn', 'scars', 'in', 'the', 'Andes', 'Mountains', 'from', 'last', 'season', '.'], 'ner_tags': [0, 0, 0, 0, 2, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Display', 'the', 'latest', 'crop', 'types', 'in', 'Israel', 'observed', 'this', 'Friday', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]}\n",
      "{'tokens': ['Show', 'recent', 'flooding', 'events', 'in', 'Sao', 'Paulo', ',', 'Brazil', ',', 'from', 'two', 'weeks', 'ago', '.'], 'ner_tags': [0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 0]}\n",
      "{'tokens': ['Highlight', 'burn', 'scars', 'in', 'Montana', 'from', 'May', '2024', '.'], 'ner_tags': [0, 0, 0, 0, 2, 0, 1, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(data):\n",
    "    print(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf82ddf-d6ee-43ba-9be7-ba69c56c77de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/project/training2411/kumar/miniconda/envs/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5640bf195c3241b6b21e386b0c431a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "Error in 13\n",
      "list index out of range\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "Error in 12\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "Error in 9\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, None]\n",
      "[0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "Error in 9\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "Error in 9\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "Error in 9\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, None]\n",
      "Error in 14\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "Error in 13\n",
      "list index out of range\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "Error in 12\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0]\n",
      "[0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, None]\n",
      "[0, 0, 2, 0, 1, 1, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0] [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
      "Error in 15\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "Error in 12\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "Error in 11\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, None]\n",
      "Error in 10\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 2, 2, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
      "[0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, None]\n",
      "Error in 14\n",
      "list index out of range\n",
      "[0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 2, 0, 1, 1, 0] [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True, use_cache=True, cache_dir=\"/p/project/training2411/kumar\")\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        print(label, word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            try:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx  # Update previous_word_idx inside the loop\n",
    "            except Exception as e:  # Use Exception instead of error\n",
    "                print(f\"Error in {word_idx}\")\n",
    "                print(e)\n",
    "                print(f\"{label}\")\n",
    "                continue\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, batch_size=128)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0399160-e78c-4ea1-a121-40d4163c2755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Burn',\n",
       "  'scars',\n",
       "  'in',\n",
       "  'Oregon',\n",
       "  ',',\n",
       "  'noted',\n",
       "  'on',\n",
       "  'April',\n",
       "  '17',\n",
       "  ',',\n",
       "  '2023',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0],\n",
       " 'input_ids': [0,\n",
       "  7960,\n",
       "  26172,\n",
       "  11,\n",
       "  4316,\n",
       "  2156,\n",
       "  1581,\n",
       "  15,\n",
       "  587,\n",
       "  601,\n",
       "  2156,\n",
       "  291,\n",
       "  1922,\n",
       "  479,\n",
       "  2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 1, 0, -100]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace9dbb2-2485-431a-8f3c-fc61f24043b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    use_cache=True,\n",
    "    cache_dir=\"/p/project/training2411/kumar\",\n",
    "    num_labels=len(unique_labels) # This should match your total number of NER tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61b17512-5dc1-4158-9ea6-4012523df45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.051451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.231514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.037117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.022357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.015271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=0.0453031246478741, metrics={'train_runtime': 11.4361, 'train_samples_per_second': 86.568, 'train_steps_per_second': 11.367, 'total_flos': 8461600380360.0, 'train_loss': 0.0453031246478741, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming you have a train split\n",
    "    eval_dataset=tokenized_datasets,  # Assuming you have a train split\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a429170d-b19b-4d38-8581-ba441dea4399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_roberta_ner/tokenizer_config.json',\n",
       " './finetuned_roberta_ner/special_tokens_map.json',\n",
       " './finetuned_roberta_ner/vocab.json',\n",
       " './finetuned_roberta_ner/merges.txt',\n",
       " './finetuned_roberta_ner/added_tokens.json',\n",
       " './finetuned_roberta_ner/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./finetuned_roberta_ner')\n",
    "tokenizer.save_pretrained('./finetuned_roberta_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba939e5e-4cf5-452e-9bd0-0434aad6e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   230,  6884,  3505,    11,  2809, 17373,   179,   883,   587,\n",
      "             6,   291,  1922,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input_ids tensor([[    0,   230,  6884,  3505,    11,  2809, 17373,   179,   883,   587,\n",
      "             6,   291,  1922,     4,     2]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Crop types in Spain durin 23 April, 2023.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "print(inputs)\n",
    "for k, v in inputs.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e09d3b-d18b-42c9-b2c3-4a995a62fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c098df6b-3ded-44f9-a523-88b246452609",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Crop types in Spain as of yesterday.\"\n",
    "ner_classes = ner_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4877b1d3-d16c-4228-acbe-b5c6851a4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.99953103,\n",
       "  'index': 1,\n",
       "  'word': 'Ä C',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9995999,\n",
       "  'index': 2,\n",
       "  'word': 'rop',\n",
       "  'start': 1,\n",
       "  'end': 4},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9996145,\n",
       "  'index': 3,\n",
       "  'word': 'Ä types',\n",
       "  'start': 5,\n",
       "  'end': 10},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9985916,\n",
       "  'index': 4,\n",
       "  'word': 'Ä in',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999062,\n",
       "  'index': 5,\n",
       "  'word': 'Ä Spain',\n",
       "  'start': 14,\n",
       "  'end': 19},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9990376,\n",
       "  'index': 6,\n",
       "  'word': 'Ä as',\n",
       "  'start': 20,\n",
       "  'end': 22},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9990075,\n",
       "  'index': 7,\n",
       "  'word': 'Ä of',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9964805,\n",
       "  'index': 8,\n",
       "  'word': 'Ä yesterday',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.8945812,\n",
       "  'index': 9,\n",
       "  'word': '.',\n",
       "  'start': 35,\n",
       "  'end': 36}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1d48c11-870d-4ee1-8b76-18e6ccddb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOCATION': 'Spain'}\n"
     ]
    }
   ],
   "source": [
    "def ner_to_dict(ner_result):\n",
    "    result = dict()\n",
    "    for item in ner_result:\n",
    "        if item['entity'] in [\"LABEL_1\", \"LABEL_2\"]:\n",
    "            if item['entity'] in result.keys():\n",
    "                result[item['entity']] += item['word']\n",
    "            else:\n",
    "                result[item['entity']] = \"\"\n",
    "                result[item['entity']] += item['word']\n",
    "    old_keys = list(result.keys())\n",
    "    \n",
    "    for key in old_keys:\n",
    "        new_key = unique_labels[int(key[-1])]\n",
    "        result[new_key] = result[key]\n",
    "        del result[key]\n",
    "    \n",
    "    for key, val in result.items():\n",
    "        result[key] = val.replace('Ä ', ' ')[1:]\n",
    "\n",
    "    return result\n",
    "\n",
    "print(ner_to_dict(ner_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f365933b-96c1-4bee-8516-e74b9bd5e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOCATION': 'Spain', 'DATE': '23 April, 2023'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0296834-1ac3-44a8-8de4-8ad59462ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
