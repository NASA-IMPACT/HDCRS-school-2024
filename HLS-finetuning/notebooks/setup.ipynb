{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d4f62d",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "1. You will be working off of terminal for this chapter.\n",
    "2. Change your working directory: ```cd /p/project/training2411/$USER/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca1130-19c1-4bc4-bfbe-a1c1d087c09d",
   "metadata": {},
   "source": [
    "# Install Monda using Miniconda\n",
    "You will be using Python and TensorFlow for training. For this you need to create an environment you can use across nodes. You will be using Miniconda to create a Python virtual environment for your experiments.\n",
    "\n",
    "## Training using high performance computing and TensorFlow\n",
    "1. Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "2. Install conda using the Miniconda script you downloaded\n",
    "3. Make sure you can use the Python from the conda environment\n",
    "4. Create a virtual environment\n",
    "5. Update environment variables\n",
    "6. Folder structure and files you will be using\n",
    "7. Update configuration for training\n",
    "8. Update batch_job file.\n",
    "9. Submit job\n",
    "10. Check progress\n",
    "11. Test saved model\n",
    "\n",
    "## Use cloud computing for inferencing\n",
    "1. Push trained model to AWS environment using boto3 (share credentials before this step)\n",
    "2. Deploy model and establish an API\n",
    "3. Interact with API endpoint to get inferences from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771ee6a-3231-4174-a1de-5881d7a5d9a2",
   "metadata": {},
   "source": [
    "## Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c6643",
   "metadata": {},
   "source": [
    "**Note: the following commands are to be run in a terminal shell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e44b4-2101-42e0-8260-7cb075ea306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70164300-f584-4d32-9938-3ea6efb4728e",
   "metadata": {},
   "source": [
    "## Install conda using the downloaded Miniconda shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18c800-4ad6-42a6-b338-b1c72a1a8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 770 Miniconda3-latest-Linux-x86_64.sh; ./Miniconda3-latest-Linux-x86_64.sh\n",
    "# Where to install miniconda (after the installation, it will ask): `/p/project/training2206/$USER/miniconda3` \n",
    "# Do you wish the installer to Initialize miniconda3? Yes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783e3d5-1a5e-4a7a-98b6-c3f1574bc67d",
   "metadata": {},
   "source": [
    "### Check if the installation updated your bashrc to automatically use Python from conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4600-e356-4613-9f2e-69a2145339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b96b9-db5b-40fd-8422-43bbac01dda2",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "All of the project-related files are located at `/p/project/training2411/$USER/`. (User is the environment variable with your user name. You can check what it is set as using `echo $USER`)\n",
    "\n",
    "Change directory to the aforementioned directory.\n",
    "\n",
    "Check for `HDCRS-school-2024` folder in the directory. If it is not present, you can use Git to download it from https://github.com/nasa-impact/pixel-detector using `git clone https://github.com/nasa-impact/HDCRS-school-2024.git`\n",
    "\n",
    "Once cloned, change the directory to the pixel-detector folder using `cd HDCRS-school-2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fe275",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/nasa-impact/HDCRS-school-2024.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cf13a-137d-4f5c-b2cd-610b95371b63",
   "metadata": {},
   "source": [
    "## Create virtual environment\n",
    "You will use the Python from the conda environment to create a virtual environment which will be used throughout.\n",
    "\n",
    "In some cases, conda might not be activated after installation. You can just refresh your bash terminal using `exec bash`, and it should enable the conda environment for you.\n",
    "\n",
    "Once in the conda environment, you can create a new Python virtual environment using `python -m venv .venv`\n",
    "\n",
    "Then you will use the environment you just created using `source .venv/bin/activate`\n",
    "\n",
    "Once the environment is activated, you will need to make sure you are starting from scratch. To make sure no other modules are installed, use `module purge` to remove all the unwanted modules.\n",
    "\n",
    "You can then install the requirements using `pip install -r requirements.txt`\n",
    "\n",
    "This will install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6991435",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857057d-6d7c-4a6d-b283-875a8eee8efe",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "For the purposes of this hands-on, data prepared data will be downloaded from Huggingface. To use custom data, please follow the details shared in [this doc](https://docs.google.com/document/d/1ExBT7lYuS-AvSfkQz8BK22l1h8n5a5Ucx9cAjDti2eY/edit).\n",
    "\n",
    "If pre-processed data is desired, there are two options available:\n",
    "| Model | Data |\n",
    "|-------|------|\n",
    "|[Burn Scars model](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-burn-scar)| [Burn Scars data](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars) |\n",
    "| [Multi-temporal Crop Classification](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification) | [Multi-temporal Crop Classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification) |\n",
    "\n",
    "The base model is also available in [huggingface as Prithvi](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
