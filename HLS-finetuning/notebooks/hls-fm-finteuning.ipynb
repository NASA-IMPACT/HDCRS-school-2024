{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb85522-53a4-43fb-95bc-b37d953662c4",
   "metadata": {},
   "source": [
    "# HLS Foundation Model Finetuning notebook\n",
    "\n",
    "This notebook demonstrates the steps to fintune the HLS foundation model (A.K.A Prithvi) which is trained using HLSL30 and HLSS30 datasets. \n",
    "\n",
    "Note: Entierty of this notebook is desigend to work well within the Julich Supercomputing Center (JSC).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HLS Foundation OS from github.\n",
    "! git clone https://github.com/nasa-impact/hls-foundation-os.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9afa3-5424-42dc-bfcb-800df0435f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!cd hls-foundation-os\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Create directories needed for data, model, and config preparations\n",
    "!mkdir datasets\n",
    "!mkdir models\n",
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c584a",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "For this hands-on session, Burn Scars example will be used for fine-tuning. All of the data and pre-trained models are available in Huggingface. Huggingface packages and git will be utilized to download, and prepare datasets and pretrained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b316d",
   "metadata": {},
   "source": [
    "Note: Git Large File Storage (git LFS) is utilized to download larger files from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570d5ed-b59b-4cb0-ab23-511a89ecadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install git lfs\n",
    "! sudo apt-get install git-lfs; git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4f1ad",
   "metadata": {},
   "source": [
    "### Download HLS Burn Scars dataset from Huggingface: https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c4e10-15cd-4f52-8dfe-ec04074efe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cd datasets; git clone https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars; tar -xvzf hls_burn_scars/hls_burn_scars.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef75557",
   "metadata": {},
   "source": [
    "## Download config and Pre-trained model\n",
    "\n",
    "The HLS Foundation Model (pre-trained model), and configuration for Burn Scars downstream task are available in Huggingface. We use `huggingface_hub` python package to download the files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BUCKET_NAME = '<your-bucket-name>' # Replace this with the bucket name available from http://smd-ai-workshop-creds-webapp.s3-website-us-east-1.amazonaws.com/ \n",
    "CONFIG_PATH = './configs'\n",
    "DATASET_PATH = './datasets'\n",
    "MODEL_PATH = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d6b5d-d58d-4b90-a952-6179c255280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download config file from huggingface\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"ibm-nasa-geospatial/Prithvi-100M-burn-scar\", filename=\"burn_scars_Prithvi_100M.py\", local_dir='./configs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f74b4d",
   "metadata": {},
   "source": [
    "Note: The configuration file in Huggingface have place holders and won't be directly usable for fine-tuning. Placeholder values need to be updated for individual usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b5c5a-9510-4d3f-8b11-dab8fdd8ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model file from huggingface\n",
    "hf_hub_download(repo_id=\"ibm-nasa-geospatial/Prithvi-100M\", filename=\"Prithvi_100M.pt\", local_dir='./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90acf449",
   "metadata": {},
   "source": [
    "*Warning: * Before running the remaining cells please update the details in the configuration file as mentioned below:\n",
    "\n",
    "1. Update line number 13 from `data_root = '<path to data root>'` to `data_root = '/p/project/training2411/<user>'`. Note: Please replace `<user>` with your username\n",
    "2. Update line number 41 from `pretrained_weights_path = '<path to pretrained weights>'` to `pretrained_weights_path = f\"{data_root}/models/Prithvi_100M.pt\"`. This provides the pre-trained model path to the train script.\n",
    "3. Update line number 53 from `experiment = '<experiment name>'` to `experiment = 'burn_scars'` or your choice of experiment name.\n",
    "4. Update line number 54 from `project_dir = '<project directory name>'` to `project_dir = 'v1'` or your choice of project directory name. \n",
    "5. Save the config file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d17936-ef79-4293-9d2d-74b2a60557ea",
   "metadata": {},
   "source": [
    "# Submit Training Job\n",
    "In the `train_job.sh` you can specify the number of nodes you want to use for training. As an example, you are going to use 2 nodes for training.\n",
    "\n",
    "Check details of the training job:\n",
    "\n",
    "`cat /p/project/training2411/$USER/HDCRS-school-2024/train_job.sh`\n",
    "\n",
    "*Note: Please replace <identifier> with a proper name for the run, <user> with your username, and <config file path> with the full configuration file path and save the file.*\n",
    "\n",
    "The training job can then be submitted using the `sbatch` command. Like so: `sbatch train_job.sh`\n",
    "\n",
    "Once submitted, two new files will be created by the process: `output.out` and `error.err`. `output.out` will contain details of the output from your processes, and `error.err` will provide details on any errors from the scripts. Once the job is submitted and the files are created, you can check for updates simply by using `tail -f output.out error.err`. (Any warnings, automated messages, and errors are tracked in the `error.err` file while only the [ed. note: incomplete sentence]\n",
    "\n",
    "You can see how good or bad the model training is by watching the loss outputs in `output.out`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a1f02",
   "metadata": {},
   "source": [
    "# Uploading the Model to a Cloud Environment\n",
    "\n",
    "After the model is finished training, the model is stored in the location specified in your config file `/p/project/training2206/<username>/HDCRS-school-2024/models/<experiment>/best_mIoU_epoch_*.pth`. `*` is the latest best performing epoch number. You will be taking this model and pushing it to an S3 bucket using `boto3` and the credentials from the AWS account shared with you.\n",
    "\n",
    "## Get AWS credentials\n",
    "Account creation links should have been shared with you. Once the account is setup, you can obtain the credentials required for upload from the AWS SSO homepage.\n",
    "Please follow the steps listed below:\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `summerSchool`\n",
    "5. Click on `Command line or Programmatic access`\n",
    "6. Copy the `AWS Access Key Id`, `AWS Secret Access Key`, and `AWS session token` from the pop up\n",
    "7. Update the following script and run it in a python shell. (You can start a python shell by just typing `python` in the terminal).\n",
    "\n",
    "*Note: Please make sure the virtual environment is active while working with the python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY_ID = <Copied over from SSO login>\n",
    "AWS_SECRET_ACCESS_KEY = <Copied over from SSO login>\n",
    "AWS_SESSION_TOKEN = <Copied over from SSO login>\n",
    "\n",
    "BUCKET_NAME = 'workshop-'\n",
    "\n",
    "USER = os.environ.get('USER')\n",
    "\n",
    "def generate_federated_session():\n",
    "    \"\"\"\n",
    "    Method to generate federated session to upload the file from HPC to S3 bucket.\n",
    "    ARGs:\n",
    "        filename: Upload filename\n",
    "    Returns: \n",
    "        Signed URL for file upload \n",
    "    \"\"\"\n",
    "    return boto3.session.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            aws_session_token=AWS_SESSION_TOKEN\n",
    "        )\n",
    "\n",
    "model_filename = f\"/p/project/training2411/<username>/HDCRS-school-2024/models/<project>/<experiment>/best_mIoU_epoch_*.pth\"\n",
    "config_filename = f\"/p/project/training2411/<username>/hls-foundation-os/configs/multi_temporal_crop_classification.py\" # please replace this with path of the config file used for fine-tuning.\n",
    "\n",
    "model_basename = os.path.basename(model_filename)\n",
    "config_basename = os.path.basename(config_filename)\n",
    "\n",
    "session = generate_federated_session()\n",
    "s3_connector = session.client('s3')\n",
    "s3_connector.upload_file(model_filename, BUCKET_NAME, f\"models/{USER}_{model_basename}.pth\")\n",
    "s3_connector.upload_file(config_filename, BUCKET_NAME, f\"configs/{USER}_{config_basename}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35c27d-eba9-4c9a-a28f-292848b982b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
